{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu64I2BfSYoa5UDIVwIs8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/435wldms/essa/blob/main/%EB%B2%95%EC%9B%90%ED%8C%90%EA%B2%B0%EC%98%88%EC%B8%A1_%EB%B0%A9%ED%95%99%EC%8A%A4%ED%84%B0%EB%94%942.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 월간 데이콘 법원 판결 예측 대회"
      ],
      "metadata": {
        "id": "5t3haQcMGwl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* OB 커리큘럼에 있는 텍스트 분석에 대해 더 깊이 이해하고, 특히 실제 데이터에서의 텍스트 전처리 방법에 대해 경험해보고자 실시"
      ],
      "metadata": {
        "id": "DF0JU8I8_kFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import module & functions**"
      ],
      "metadata": {
        "id": "A5SGffKFIkZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U 'spacy[cuda-autodetect]'\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md\n",
        "!pip install numpy requests nlpaug\n",
        "!pip install torch>=1.6.0 transformers>=4.11.3 sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pzngxJpznD0e",
        "outputId": "bec67a41-3d30-4eaf-8a2d-abb2323ac005"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.1.0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-24.0 setuptools-69.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy[cuda-autodetect] in /usr/local/lib/python3.10/dist-packages (3.7.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (69.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy[cuda-autodetect]) (1.23.5)\n",
            "Collecting cupy-wheel<13.0.0,>=11.0.0 (from spacy[cuda-autodetect])\n",
            "  Downloading cupy-wheel-12.3.0.tar.gz (2.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cupy-cuda12x==12.3.0 (from cupy-wheel<13.0.0,>=11.0.0->spacy[cuda-autodetect])\n",
            "  Downloading cupy_cuda12x-12.3.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x==12.3.0->cupy-wheel<13.0.0,>=11.0.0->spacy[cuda-autodetect]) (0.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda-autodetect]) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda-autodetect]) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[cuda-autodetect]) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda-autodetect]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda-autodetect]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda-autodetect]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy[cuda-autodetect]) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy[cuda-autodetect]) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy[cuda-autodetect]) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy[cuda-autodetect]) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy[cuda-autodetect]) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy[cuda-autodetect]) (2.1.5)\n",
            "Downloading cupy_cuda12x-12.3.0-cp310-cp310-manylinux2014_x86_64.whl (81.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cupy-wheel\n",
            "  Building wheel for cupy-wheel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cupy-wheel: filename=cupy_wheel-12.3.0-py3-none-any.whl size=993 sha256=b2a0932b25e3a1a60251bc291f96e715dcf09cca914c252c85a2da730b31c2eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/f3/9d/f049ae6f63a76342d52b752cfe2eabd7b96dcc363506bb00fa\n",
            "Successfully built cupy-wheel\n",
            "Installing collected packages: cupy-cuda12x, cupy-wheel\n",
            "  Attempting uninstall: cupy-cuda12x\n",
            "    Found existing installation: cupy-cuda12x 12.2.0\n",
            "    Uninstalling cupy-cuda12x-12.2.0:\n",
            "      Successfully uninstalled cupy-cuda12x-12.2.0\n",
            "Successfully installed cupy-cuda12x-12.3.0 cupy-wheel-12.3.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.7.1) (3.7.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (69.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.23.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.5)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.7.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (1.5.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from nlpaug) (4.7.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.0.0->nlpaug) (4.12.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->nlpaug) (2023.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
            "Installing collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.11\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "import itertools\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk import ne_chunk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "import nlpaug.augmenter.word as naw\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_colwidth', 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjIciuSfnDcv",
        "outputId": "7048a52a-5d5f-4fef-a8dd-62cf31eeaafc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the dataset**"
      ],
      "metadata": {
        "id": "HxrdU7jlnJB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "fxhJs16OnQ7b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Preprocessing"
      ],
      "metadata": {
        "id": "BLwkyaRdnIo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 전처리 전, 문장의 평균 길이 확인"
      ],
      "metadata": {
        "id": "9AdnKFiPnu2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_before_augmentation = train['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average train facts character length (before augmentation): {avg_char_before_augmentation:.0f}')\n",
        "\n",
        "avg_word_before_augmentation = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average train facts word length (before augmentation): {avg_word_before_augmentation:.0f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uc_GBZIWnrDB",
        "outputId": "78b18d1c-a7ba-4d29-fa2d-1a45cf81ed27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train facts character length (before augmentation): 1109\n",
            "Average train facts word length (before augmentation): 174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_before_augmentation = test['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average test facts character length (before augmentation): {avg_char_before_augmentation:.0f}')\n",
        "\n",
        "avg_word_before_augmentation = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average test facts word length (before augmentation): {avg_word_before_augmentation:.0f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e740NBPDn6o3",
        "outputId": "011fc6e6-b521-4c71-ebda-8dc59bb4f6eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average test facts character length (before augmentation): 1111\n",
            "Average test facts word length (before augmentation): 174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) will 통일 (변경을 안하면 불용어처리에서 지워짐)\n",
        "* 부가 설명: TF-IDF 벡터화를 수행할 때, 보통 불용어(stop words)를 제거하는 과정이 필요합니다. 불용어는 문서에서 자주 등장하지만 정보를 제공하지 않는 단어들로, 예를 들어 'the', 'and', 'is'와 같은 단어들이 해당됩니다. 이러한 불용어들은 일반적으로 문맥을 이해하는 데 도움이 되지 않기 때문에 제거됩니다. -> will은 미래의 의미도 가지고 있기 때문"
      ],
      "metadata": {
        "id": "E-TCeTi3o4Ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_apply = ['first_party', 'second_party', 'facts']"
      ],
      "metadata": {
        "id": "0qjx4wPVpRXT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns_to_apply:\n",
        "    train[column] = train[column].apply(lambda x: re.sub(r'\\bWill\\b', 'Willn', x))\n",
        "\n",
        "for column in columns_to_apply:\n",
        "    test[column] = test[column].apply(lambda x: re.sub(r'\\bWill\\b', 'Willn', x))"
      ],
      "metadata": {
        "id": "VKScKw7kpjUg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        " re.sub() 함수는 정규 표현식을 사용하여 문자열에서 패턴을 찾아 다른 문자열로 대체하는 파이썬의 문자열 메서드\n",
        "\n",
        " \\b는 단어 경계를 나타내며, 'Will'의 경계를 나타내기 위해 사용됨. 따라서 'Will'이라는 단어가 완전한 단어로 존재할 때만 이름을 변경하도록 함으로써 'William'이나 'Willow'과 같은 단어는 변경되지 않음."
      ],
      "metadata": {
        "id": "ncm3lg7rqWFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) United States 통일"
      ],
      "metadata": {
        "id": "0B5Xu7ltqpml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replace_dict = {\n",
        "    'U. S. C.': ' USC ',\n",
        "    'U.S.C.': ' USC ',\n",
        "    'U.S.A.': ' USA ',\n",
        "    'U. S.': ' USA ',\n",
        "    'U.S.': ' USA ',\n",
        "    'US ': ' USA ',\n",
        "    'United States of America': ' USA ',\n",
        "    'United States': ' USA ',\n",
        "    'united states': ' USA '\n",
        "}\n",
        "for col in train.columns:\n",
        "    train[col] = train[col].replace(replace_dict, regex=True)\n",
        "\n",
        "for col in test.columns:\n",
        "    test[col] = test[col].replace(replace_dict, regex=True)"
      ],
      "metadata": {
        "id": "vb06l32UqtEa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) 공백 제거 및 대문자 + '.'+ 대문자 제거"
      ],
      "metadata": {
        "id": "gureSFf8rkcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_extra_whitespaces_func(text):\n",
        "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()"
      ],
      "metadata": {
        "id": "Rf8wN-GHrbWX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "VF70coPhrfrI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        "이 함수는 문자열에서 시작과 끝의 공백을 제거하고, 하나 이상의 연속된 공백을 ''로 대체하여 문자열의 공백을 효과적으로 정리할 수 있음."
      ],
      "metadata": {
        "id": "pZbY3_3xrNvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_Large_and_Large(text):\n",
        "    pattern1 = r'(?<=[A-Z])\\.+\\s+(?=[A-Z]+\\.)'\n",
        "    pattern2 = r'(?<=[A-Z])\\.+(?=[A-Z]+\\.)'\n",
        "    pattern3 = r'([A-Z])\\.'\n",
        "\n",
        "    result1 = re.sub(pattern1, '', text)\n",
        "    result2 = re.sub(pattern2, '', result1)\n",
        "    result3 = re.sub(pattern3, lambda match: match.group(1)+' ', result2)\n",
        "\n",
        "    return result3\n",
        "\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_Large_and_Large))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_Large_and_Large))"
      ],
      "metadata": {
        "id": "wP4UOsoYrNJp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "fVx0HE3Zs78a"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        "pattern1: 문장에서 대문자로 시작하고, 그 뒤에 하나 이상의 점이 오며, 이후에 공백과 대문자가 나오는 패턴 찾기.\n",
        "\n",
        "pattern2: 문장에서 대문자로 시작하고, 그 뒤에 하나 이상의 점이 오며, 이후에 대문자가 나오는 패턴 찾기.\n",
        "\n",
        "pattern3: 대문자 다음에 점이 오는 패턴 찾기"
      ],
      "metadata": {
        "id": "xEgH8pj0tA2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) 대문자 & 대문자인 패턴을 & 제거하고 병합"
      ],
      "metadata": {
        "id": "4bP3NwdRt1_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_And(text):\n",
        "    pattern1 = r'(?<=[A-Z])\\s+\\&\\s+(?=[A-Z])'\n",
        "    pattern2 = r'(?<=[A-Z])\\&(?=[A-Z])'\\\n",
        "\n",
        "    result1 = re.sub(pattern1, 'n', text)\n",
        "    result2 = re.sub(pattern2, 'n', result1)\n",
        "    result3 = result2.replace('&',' and ')\n",
        "\n",
        "    return result3\n",
        "\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_And))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_And))"
      ],
      "metadata": {
        "id": "xQiUYaR8uGPo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "RRrZZap9uR1y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) 한 글자로 된 대문자 제거"
      ],
      "metadata": {
        "id": "gHwTorWWuVCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_One_Large(text):\n",
        "    text = ' ' + text + ' '\n",
        "    pattern = r'(?<=\\s)[A-Z](?=\\s)'\n",
        "\n",
        "    result = re.sub(pattern, ' ', text)\n",
        "\n",
        "    return result\n",
        "\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_One_Large))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_One_Large))"
      ],
      "metadata": {
        "id": "OACXeDOIuXIH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "T3ZjFbv5uvSC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        "문장의 맨 앞과 뒤에 공백을 만들어 준 후에 양쪽 공백으로 둘러싸인 단어를 탐색"
      ],
      "metadata": {
        "id": "iVERSh39vXWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Co. Bd. Mt.와 같은 약어를 원 단어로 변경"
      ],
      "metadata": {
        "id": "2EzMC7q6vXOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Hon_rep = [r'Co\\.', r'CO\\.' , r'Bd\\.', r'Mt\\.']"
      ],
      "metadata": {
        "id": "ECMu_t3ivqly"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_values_in_column(column, pattern, replacement):\n",
        "    new_column = []\n",
        "    for item in tqdm(column):\n",
        "        new_item = re.sub(pattern, replacement, item)\n",
        "        new_column.append(new_item)\n",
        "    return new_column\n",
        "\n",
        "# 해당 패턴에 맞는 값들을 원하는 replacement 값으로 바꾸는 함수\n",
        "def replace_values_in_train(train, column_name, pattern, replacement):\n",
        "    train[column_name] = replace_values_in_column(train[column_name], pattern, replacement)\n",
        "    return train\n",
        "\n",
        "def replace_values_in_test(test, column_name, pattern, replacement):\n",
        "    test[column_name] = replace_values_in_column(test[column_name], pattern, replacement)\n",
        "    return test\n",
        "\n",
        "# train 데이터프레임에서 각 컬럼들에 대해 값을 바꿈\n",
        "for pattern, replacement in zip(Hon_rep, [' Company ', ' Company ', ' Building ', ' Mount ']):\n",
        "    train = replace_values_in_train(train, 'first_party', pattern, replacement)\n",
        "    train = replace_values_in_train(train, 'second_party', pattern, replacement)\n",
        "    train = replace_values_in_train(train, 'facts', pattern, replacement)\n",
        "\n",
        "# test 데이터프레임에서 각 컬럼들에 대해 값을 바꿈\n",
        "for pattern, replacement in zip(Hon_rep, [' Company ', ' Company ', ' Building ', ' Mount ']):\n",
        "    test = replace_values_in_test(test, 'first_party', pattern, replacement)\n",
        "    test = replace_values_in_test(test, 'second_party', pattern, replacement)\n",
        "    test = replace_values_in_test(test, 'facts', pattern, replacement)"
      ],
      "metadata": {
        "id": "tm0lMYiewEkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "eF6HKLOYwNjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) INC 처리 (회사명이라 필요 없음)"
      ],
      "metadata": {
        "id": "PemneL9Sw9lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_dot_after_inc1(text):\n",
        "    pattern = r'Inc\\.\\s+([^A-Z\\s]+)'\n",
        "\n",
        "    def replace_dot(match):\n",
        "        return match.group(0).replace(\".\", \"\")\n",
        "\n",
        "    result = re.sub(pattern, replace_dot, text)\n",
        "    return result\n",
        "\n",
        "def remove_dot_after_inc2(text):\n",
        "    pattern = r'Inc\\.([^A-Z\\s]+)'\n",
        "\n",
        "    def replace_dot2(match):\n",
        "        return match.group(0).replace(\".\", \"\")\n",
        "\n",
        "    result = re.sub(pattern, replace_dot2, text)\n",
        "    return result\n",
        "\n",
        "def remove_dot_after_ltd1(text):\n",
        "    pattern = r'Ltd\\.\\s+([^A-Z\\s]+)'\n",
        "\n",
        "    def replace_dot3(match):\n",
        "        return match.group(0).replace(\".\", \"\")\n",
        "\n",
        "    result = re.sub(pattern, replace_dot3, text)\n",
        "    return result\n",
        "\n",
        "def remove_dot_after_ltd2(text):\n",
        "    pattern = r'Ltd\\.([^A-Z\\s]+)'\n",
        "\n",
        "    def replace_dot4(match):\n",
        "        return match.group(0).replace(\".\", \"\")\n",
        "\n",
        "    result = re.sub(pattern, replace_dot4, text)\n",
        "    return result\n",
        "\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc1))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc2))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd1))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd2))\n",
        "\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc1))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_inc2))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd1))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_dot_after_ltd2))"
      ],
      "metadata": {
        "id": "LI7tB8ZCxEs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "kymHoZRkxNkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Augmentation"
      ],
      "metadata": {
        "id": "LGKRngakmnHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) facts에서 명사 추출"
      ],
      "metadata": {
        "id": "pNRkXQI5mr8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "stXB-iDvmw9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noun_str_train = []\n",
        "noun_str_test = []\n",
        "\n",
        "noun_str_train = train['facts'].apply(lambda x: [chunk.text for chunk in nlp(x).noun_chunks]).tolist()\n",
        "noun_str_test = test['facts'].apply(lambda x: [chunk.text for chunk in nlp(x).noun_chunks]).tolist()"
      ],
      "metadata": {
        "id": "mFqx_OCRm2Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        "spaCy의 nlp() 함수에 전달하여 텍스트를 자연어 처리한 뒤, noun_chunks 속성을 사용하여 명사구를 추출하고, 추출된 명사구들의 텍스트를 리스트로 저장"
      ],
      "metadata": {
        "id": "7DG170O-nRix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) contextual word embedding을 사용해 데이터 보강\n",
        "\n",
        "* contextual word embedding: 단어를 벡터로 표현하는 기술 중 하나로 각 단어를 고차원의 벡터로 표현하면서, 해당 단어의 의미와 문맥을 반영."
      ],
      "metadata": {
        "id": "EiaCDZVrnh4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_0 = train[train['first_party_winner'] == 0].copy()\n",
        "train_0 = pd.concat([train_0]*5, ignore_index=True)\n",
        "train_0['number'] = train_0['ID'].str.replace('TRAIN_', '').astype(int)\n",
        "train_0 = train_0.sort_values('number').reset_index(drop=True)\n",
        "train_0 = train_0.drop('number', axis=1)"
      ],
      "metadata": {
        "id": "YChsUgwGoPWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1 = train[train['first_party_winner'] == 1].copy()\n",
        "train_1 = pd.concat([train_1]*5, ignore_index=True)\n",
        "train_1['number'] = train_1['ID'].str.replace('TRAIN_', '').astype(int)\n",
        "train_1 = train_1.sort_values('number').reset_index(drop=True)\n",
        "train_1 = train_1.drop('number', axis=1)"
      ],
      "metadata": {
        "id": "Bf6GS8csoWkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        "first_party_winner가 0/1인 경우에 한해서 데이터프레임을 5번 복제한 형태를 생성, number 열 기준으로 데이터프레임 정렬"
      ],
      "metadata": {
        "id": "q6PtVg9Gz0mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set() 함수: 중복된 요소 제거\n",
        "\n",
        "noun_str_train_2 = []\n",
        "\n",
        "for i in range(len(noun_str_train)):\n",
        "    noun_str_train_2.append(list(set(noun_str_train[i])))"
      ],
      "metadata": {
        "id": "yy1uS30qoYee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_facts_train_0 = []\n",
        "final_facts_train_1 = []\n",
        "\n",
        "indices_train_0 = train[train['first_party_winner'] == 0].index.tolist()\n",
        "indices_train_1 = train[train['first_party_winner'] == 1].index.tolist()"
      ],
      "metadata": {
        "id": "wRlb2dmKob6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(indices_train_0):\n",
        "    aug = naw.SynonymAug(aug_src='wordnet', stopwords=noun_str_train_2[i])\n",
        "    final_facts_train_0.append([train['facts'][i]])\n",
        "    for j in range(4):\n",
        "        augmented_train_0 = aug.augment(train['facts'][i],1,8)\n",
        "        final_facts_train_0.append(augmented_train_0)\n",
        "\n",
        "for i in tqdm(indices_train_1):\n",
        "    aug = naw.SynonymAug(aug_src='wordnet', stopwords=noun_str_train_2[i])\n",
        "    final_facts_train_1.append([train['facts'][i]])\n",
        "    for j in range(4):\n",
        "        augmented_train_1 = aug.augment(train['facts'][i],1,8)\n",
        "        final_facts_train_1.append(augmented_train_1)"
      ],
      "metadata": {
        "id": "QQVk8YAHokkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        "각 행에 대해 SynonymAugmentation을 사용하여 텍스트 데이터를 보강함 (WordNet을 기반으로한 동의어 증강)\n",
        "\n",
        "'first_party_winner' 열이 0인 행들과 1인 행들에 대해 보강된 데이터를 각각 final_facts_train_0와 final_facts_train_1에 저장함"
      ],
      "metadata": {
        "id": "WHCu6OjN2QC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_facts_train_0 = [item for sublist in final_facts_train_0 for item in sublist]\n",
        "final_facts_train_1 = [item for sublist in final_facts_train_1 for item in sublist]"
      ],
      "metadata": {
        "id": "aMRIru9Homw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_0['facts'] = final_facts_train_0\n",
        "train_1['facts'] = final_facts_train_1\n",
        "train = pd.concat([train_0,train_1]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "pYy-VMxAo0g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['number'] = train['ID'].str.replace('TRAIN_', '').astype(int)\n",
        "train = train.sort_values('ID').reset_index(drop=True)\n",
        "train = train.drop('number', axis=1)\n",
        "train['ID'] = train.index.map(lambda x: f'TRAIN_{x:04}')"
      ],
      "metadata": {
        "id": "ekJtMdaNo4Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        "보강된 데이터를 기존의 train 데이터프레임에 추가하고, 데이터프레임을 재구성하는 작업을 수행"
      ],
      "metadata": {
        "id": "68335w1L44ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) et ~ 제거"
      ],
      "metadata": {
        "id": "b8y26KTE5Zt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replace_dict = {\n",
        "\n",
        "    'et. al.': ' ',\n",
        "    'et. al': ' ',\n",
        "    'et al.': ' ',\n",
        "    'et al': ' ',\n",
        "\n",
        "    'at. al.': ' ',\n",
        "    'at. al': ' ',\n",
        "    'at al.': ' ',\n",
        "    'at al': ' ',\n",
        "\n",
        "    'et. ux.': ' ',\n",
        "    'et. ux': ' ',\n",
        "    'et ux.': ' ',\n",
        "    'et ux': ' ',\n",
        "\n",
        "    'et. ex.': ' ',\n",
        "    'et. ex': ' ',\n",
        "    'et ex.': ' ',\n",
        "    'et ex': ' ',\n",
        "\n",
        "    'ex. re.': ' ',\n",
        "    'ex. re': ' ',\n",
        "    'ex re.': ' ',\n",
        "    'ex re': ' ',\n",
        "\n",
        "    'et. re.': ' ',\n",
        "    'et. re': ' ',\n",
        "    'et re.': ' ',\n",
        "    'et re': ' ',\n",
        "\n",
        "    'et. seq.': ' ',\n",
        "    'et. seq': ' ',\n",
        "    'et seq.': ' ',\n",
        "    'et seq': ' ',\n",
        "\n",
        "    'et. vir.': ' ',\n",
        "    'et. vir': ' ',\n",
        "    'et vir.': ' ',\n",
        "    'et vir': ' ',\n",
        "\n",
        "    'ex. rel.': ' ',\n",
        "    'ex. rel': ' ',\n",
        "    'ex rel.': ' ',\n",
        "    'ex rel': ' ',\n",
        "    'etc' : ' '\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "_nr3h06vin9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임의 각 열에 대해 일치하는 패턴을 가진 값을 다른 값으로 대체\n",
        "\n",
        "for col in train.columns:\n",
        "    train[col] = train[col].replace(replace_dict, regex=True)\n",
        "\n",
        "for col in test.columns:\n",
        "    test[col] = test[col].replace(replace_dict, regex=True)"
      ],
      "metadata": {
        "id": "mu5p1ywBkwiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "AZFnzxl-lPfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) 영어 호칭 변경"
      ],
      "metadata": {
        "id": "oZcHImFNlZIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Honor = [r'Mr.', r'Mrs.', r'Miss.', r'Dr.', r'Rev.', r'Prof.', r'Capt.', r'Sgt.', r'St.', r'Sr.', r'Jr.', r'Ms.', r'No.']\n",
        "\n",
        "for i in tqdm(range(len(train))):\n",
        "    for k in Honor:\n",
        "        train.loc[i, 'facts'] = \" \" + train.loc[i, 'facts'] + \" \"\n",
        "        train.loc[i, 'facts'] = train.loc[i, 'facts'].replace(k,' ')\n",
        "        train.loc[i, 'facts'] = re.sub(r'\\s+', ' ', train.loc[i, 'facts'])\n",
        "        train.loc[i, 'facts'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'facts'])\n",
        "\n",
        "        train.loc[i, 'first_party'] = \" \" + train.loc[i, 'first_party'] + \" \"\n",
        "        train.loc[i, 'first_party'] = train.loc[i, 'first_party'].replace(k,' ')\n",
        "        train.loc[i, 'first_party'] = re.sub(r'\\s+', ' ', train.loc[i, 'first_party'])\n",
        "        train.loc[i, 'first_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'first_party'])\n",
        "\n",
        "        train.loc[i, 'second_party'] = \" \" + train.loc[i, 'second_party'] + \" \"\n",
        "        train.loc[i, 'second_party'] = train.loc[i, 'second_party'].replace(k,' ')\n",
        "        train.loc[i, 'second_party'] = re.sub(r'\\s+', ' ', train.loc[i, 'second_party'])\n",
        "        train.loc[i, 'second_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", train.loc[i, 'second_party'])\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(test))):\n",
        "    for k in Honor:\n",
        "        test.loc[i, 'facts'] = \" \" + test.loc[i, 'facts'] + \" \"\n",
        "        test.loc[i, 'facts'] = test.loc[i, 'facts'].replace(k,' ')\n",
        "        test.loc[i, 'facts'] = re.sub(r'\\s+', ' ', test.loc[i, 'facts'])\n",
        "        test.loc[i, 'facts'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'facts'])\n",
        "\n",
        "        test.loc[i, 'first_party'] = \" \" + test.loc[i, 'first_party'] + \" \"\n",
        "        test.loc[i, 'first_party'] = test.loc[i, 'first_party'].replace(k,' ')\n",
        "        test.loc[i, 'first_party'] = re.sub(r'\\s+', ' ', test.loc[i, 'first_party'])\n",
        "        test.loc[i, 'first_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'first_party'])\n",
        "\n",
        "        test.loc[i, 'second_party'] = \" \" + test.loc[i, 'second_party'] + \" \"\n",
        "        test.loc[i, 'second_party'] = test.loc[i, 'second_party'].replace(k,' ')\n",
        "        test.loc[i, 'second_party'] = re.sub(r'\\s+', ' ', test.loc[i, 'second_party'])\n",
        "        test.loc[i, 'second_party'] = re.sub(r\"^\\s+|\\s+$\", \"\", test.loc[i, 'second_party'])"
      ],
      "metadata": {
        "id": "iM3a75lKlioS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        "먼저, 각 문자열의 앞뒤에 공백을 추가한 후, 각 명칭에 대해 해당 단어를 공백으로 대체.\n",
        "\n",
        "연속된 공백을 단일 공백으로 대체하고, 문자열의 앞뒤 공백을 제거하여 정리."
      ],
      "metadata": {
        "id": "EdSaq16HmCcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))"
      ],
      "metadata": {
        "id": "bzqq-TRgmEG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) 문장분할하기"
      ],
      "metadata": {
        "id": "inUqizsOnmqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "jSyKIws6nvXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nlp(text).sents = 해당 텍스트를 문장 단위로 분할한 결과를 나타내는 spaCy의 generator\n",
        "sentence_train = []\n",
        "sentence_test = []\n",
        "\n",
        "train['facts'] = train['facts'].apply(lambda text: ' '.join([str(sent) + ' [SEP]' for sent in nlp(text).sents]))\n",
        "test['facts'] = test['facts'].apply(lambda text: ' '.join([str(sent) + ' [SEP]' for sent in nlp(text).sents]))"
      ],
      "metadata": {
        "id": "PuF5fH7nnxAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 코드 부연 설명\n",
        "\n",
        " 각 'facts' 열의 텍스트는 문장 단위로 분할되어 '[SEP]' 토큰을 기준으로 구분된 문자열로 변환"
      ],
      "metadata": {
        "id": "BWbTaFZepyCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Preprocess Dataset"
      ],
      "metadata": {
        "id": "RFXBK-Lgp-1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**전처리 전 평균 길이 확인**"
      ],
      "metadata": {
        "id": "7Uc7EAADqFFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_before_preprocessing = train['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average train facts character length (before preprocessing): {avg_char_before_preprocessing:.0f}')\n",
        "\n",
        "avg_word_before_preprocessing = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average train facts word length (before preprocessing): {avg_word_before_preprocessing:.0f}')"
      ],
      "metadata": {
        "id": "IRkh27ErqVnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_before_preprocessing = test['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average test facts character length (before preprocessing): {avg_char_before_preprocessing:.0f}')\n",
        "\n",
        "avg_word_before_preprocessing = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average test facts word length (before preprocessing): {avg_word_before_preprocessing:.0f}')"
      ],
      "metadata": {
        "id": "frQUtA5GqcOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 전처리 함수"
      ],
      "metadata": {
        "id": "D9S_MzYKtT52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html_tags_func(text):\n",
        "    return BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "\n",
        "def remove_url_func(text):\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "\n",
        "def remove_accented_chars_func(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "\n",
        "def remove_punctuation_func(text):\n",
        "    return re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
        "\n",
        "\n",
        "def remove_irr_char_func(text):\n",
        "    return re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "\n",
        "\n",
        "def remove_extra_whitespaces_func(text):\n",
        "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
        "\n",
        "def remove_english_stopwords_func(text):\n",
        "    t = [token for token in text if token.lower() not in stopwords.words(\"english\")]\n",
        "    text = ' '.join(t)\n",
        "    return text\n",
        "\n",
        "def norm_lemm_v_a_func(text):\n",
        "    words1 = word_tokenize(text)\n",
        "    text1 = ' '.join([WordNetLemmatizer().lemmatize(word, pos='v') for word in words1])\n",
        "    words2 = word_tokenize(text1)\n",
        "    text2 = ' '.join([WordNetLemmatizer().lemmatize(word, pos='a') for word in words2])\n",
        "    return text2\n",
        "\n",
        "def remove_single_char_func(text, threshold=1):\n",
        "    words = word_tokenize(text)\n",
        "    text = ' '.join([word for word in words if len(word) > threshold])\n",
        "    return text"
      ],
      "metadata": {
        "id": "KIvZOQlQtaPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "text cleaning"
      ],
      "metadata": {
        "id": "ypX6J8S3wRtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.str.lower())\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_html_tags_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_url_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_accented_chars_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_punctuation_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_irr_char_func))\n",
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "train['facts'] = train['facts'].apply(lambda x: x.replace('cls', 'CLS').replace('sep', 'SEP'))"
      ],
      "metadata": {
        "id": "iKOTuo0MwTSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.str.lower())\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_html_tags_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_url_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_accented_chars_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_punctuation_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_irr_char_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_extra_whitespaces_func))\n",
        "test['facts'] = test['facts'].apply(lambda x: x.replace('cls', 'CLS').replace('sep', 'SEP'))"
      ],
      "metadata": {
        "id": "QXipB--JwWFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenization"
      ],
      "metadata": {
        "id": "01gEwHU9wist"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(word_tokenize))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(word_tokenize))"
      ],
      "metadata": {
        "id": "D3Jyxkc1wks1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stop words (위 함수 적용)"
      ],
      "metadata": {
        "id": "G9FoJT-8wnUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_english_stopwords_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_english_stopwords_func))"
      ],
      "metadata": {
        "id": "qw-en1fnwj61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalization"
      ],
      "metadata": {
        "id": "qT8BVVlxwtb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(norm_lemm_v_a_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(norm_lemm_v_a_func))"
      ],
      "metadata": {
        "id": "2TvhXfQUwvA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing single characters"
      ],
      "metadata": {
        "id": "02Kv4-I4w3Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[columns_to_apply] = train[columns_to_apply].apply(lambda x: x.apply(remove_single_char_func))\n",
        "test[columns_to_apply] = test[columns_to_apply].apply(lambda x: x.apply(remove_single_char_func))"
      ],
      "metadata": {
        "id": "E57-Pntgw6Q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리 후 문장 평균 길이 확인"
      ],
      "metadata": {
        "id": "zYpCYKsGxGjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_after_preprocessing = train['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average train facts character length (after preprocessing): {avg_char_after_preprocessing:.0f}')\n",
        "\n",
        "avg_word_after_preprocessing = train['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average train facts word length (after preprocessing): {avg_word_after_preprocessing:.0f}')"
      ],
      "metadata": {
        "id": "HT_nO7vnxMFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_char_after_preprocessing = test['facts'].apply(lambda x: len(str(x))).mean()\n",
        "print(f'Average test facts character length (after preprocessing): {avg_char_after_preprocessing:.0f}')\n",
        "\n",
        "avg_word_after_preprocessing = test['facts'].apply(lambda x: len(str(x).split())).mean()\n",
        "print(f'Average test facts word length (after preprocessing): {avg_word_after_preprocessing:.0f}')"
      ],
      "metadata": {
        "id": "W0p1CptPxPRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Text exploration"
      ],
      "metadata": {
        "id": "iYzhhH2w8NZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  가장 일반적으로 등장하는 단어와 가장 드물게 등장하는 단어를 추출하는 함\n",
        "def most_common_word_func(text, n_words=25):\n",
        "    words = word_tokenize(text)\n",
        "    fdist = FreqDist(words)\n",
        "\n",
        "    n_words = n_words\n",
        "\n",
        "    df_fdist = pd.DataFrame({'Word': fdist.keys(),\n",
        "                             'Frequency': fdist.values()})\n",
        "    df_fdist = df_fdist.sort_values(by='Frequency', ascending=False).head(n_words)\n",
        "\n",
        "    return df_fdist\n",
        "\n",
        "def label_func(winner):\n",
        "    if winner == 0:\n",
        "        return 'second_party'\n",
        "    elif winner == 1:\n",
        "        return 'first_party'\n",
        "\n",
        "def least_common_word_func(text, n_words=25):\n",
        "    words = word_tokenize(text)\n",
        "    fdist = FreqDist(words)\n",
        "\n",
        "    n_words = n_words\n",
        "\n",
        "    df_fdist = pd.DataFrame({'Word': fdist.keys(),\n",
        "                             'Frequency': fdist.values()})\n",
        "    df_fdist = df_fdist.sort_values(by='Frequency', ascending=False).tail(n_words)\n",
        "\n",
        "    return df_fdist"
      ],
      "metadata": {
        "id": "ILKyh1pt8RA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = pd.concat([train.iloc[:,:-1],test]).reset_index(drop=True)\n",
        "train_copy = train.copy()\n",
        "test_copy = test.copy()"
      ],
      "metadata": {
        "id": "fRCxDCBQ8s8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 흔한 단어들\n",
        "\n",
        "text_corpus = df_copy['facts'].str.cat(sep=' ')\n",
        "\n",
        "df_most_common_words_text_corpus = most_common_word_func(text_corpus)\n",
        "\n",
        "df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])].head(10)"
      ],
      "metadata": {
        "id": "B-olWDNB8yNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(11,7))\n",
        "plt.bar(df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])]['Word'],\n",
        "        df_most_common_words_text_corpus[~df_most_common_words_text_corpus['Word'].isin(['SEP', 'CLS'])]['Frequency'])\n",
        "\n",
        "plt.xticks(rotation = 45)\n",
        "\n",
        "plt.xlabel('Most common Words')\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Frequency distribution of the 25 most common words\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I_I43z1883py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "우승자 별로 어떤 단어가 자주 등장하는지 확"
      ],
      "metadata": {
        "id": "wlk_qeVU9PbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_copy['Label'] = train_copy['first_party_winner'].apply(label_func)\n",
        "\n",
        "cols = list(train_copy.columns)\n",
        "cols = [cols[-1]] + cols[:-1] # 'Label' 열을 첫 번째로 위치시키기\n",
        "train_copy = train_copy[cols]\n",
        "\n",
        "first_party = train_copy[(train_copy[\"Label\"] == 'first_party')]['facts']\n",
        "second_party = train_copy[(train_copy[\"Label\"] == 'second_party')]['facts']\n",
        "\n",
        "# 각 파티의 'facts' 열 데이터를 문자열로 결합하여 텍스트 코퍼스(말뭉치) 생성\n",
        "text_corpus_first_party = first_party.str.cat(sep=' ')\n",
        "text_corpus_second_party = second_party.str.cat(sep=' ')\n",
        "\n",
        "df_most_common_words_text_corpus_first_party = most_common_word_func(text_corpus_first_party)\n",
        "df_most_common_words_text_corpus_second_party = most_common_word_func(text_corpus_second_party)"
      ],
      "metadata": {
        "id": "YHJKleB99WXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splited_data = [df_most_common_words_text_corpus_first_party[~df_most_common_words_text_corpus_first_party['Word'].isin(['SEP', 'CLS'])],\n",
        "                df_most_common_words_text_corpus_second_party[~df_most_common_words_text_corpus_second_party['Word'].isin(['SEP', 'CLS'])]]\n",
        "\n",
        "color_list = ['green', 'red']\n",
        "title_list = ['First party', 'Second party']\n",
        "\n",
        "\n",
        "for item in range(2):\n",
        "    plt.figure(figsize=(11,7))\n",
        "    plt.bar(splited_data[item]['Word'],\n",
        "            splited_data[item]['Frequency'],\n",
        "            color=color_list[item])\n",
        "    plt.xticks(rotation = 45)\n",
        "    plt.xlabel('Most common Words')\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.title(\"Frequency distribution of the 25 most common words\")\n",
        "    plt.suptitle(title_list[item], fontsize=15)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4rWaY0QY9kwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Tokenize"
      ],
      "metadata": {
        "id": "NM1GpgfO-t9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data resampling - 데이터 불균형 보정하기 위해 진행"
      ],
      "metadata": {
        "id": "Br645tQy-7yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_02 = train.copy()\n",
        "\n",
        "train_02['first_party'], train_02['second_party'] = train_02['second_party'], train_02['first_party']\n",
        "train_02['first_party_winner'] = 1 - train_02['first_party_winner']\n",
        "\n",
        "train_02 = pd.concat([train, train_02], ignore_index=True)\n",
        "train_02['number'] = train_02['ID'].str.replace('TRAIN_', '').astype(int)\n",
        "train_02 = train_02.sort_values(['number', 'first_party_winner']).reset_index(drop=True)\n",
        "train = train_02.drop('number', axis=1)\n",
        "train['ID'] = 'TRAIN_' + train.index.map(lambda x: f'{x:04}')"
      ],
      "metadata": {
        "id": "2uFicAlS--Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Special token"
      ],
      "metadata": {
        "id": "0BwztcjO_T9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns_to_apply:\n",
        "    train[column] = train[column].apply(lambda x: re.sub(r'\\bSEP\\b', '[SEP]', x))\n",
        "\n",
        "for column in columns_to_apply:\n",
        "    test[column] = test[column].apply(lambda x: re.sub(r'\\bSEP\\b', '[SEP]', x))\n",
        "\n",
        "train['facts'] = '[CLS] ' + train['first_party'] + ' [SEP] ' + train['second_party'] + ' [SEP] ' + train['facts']\n",
        "test['facts'] = '[CLS] ' + test['first_party'] + ' [SEP] ' + test['second_party'] + ' [SEP] ' + test['facts']"
      ],
      "metadata": {
        "id": "EU6GWA_d_Vu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "csv 파일로 내보냄"
      ],
      "metadata": {
        "id": "G-a7-vzH_bHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('train_last_3.csv', index = False)\n",
        "test.to_csv('test_last_3.csv', index = False)"
      ],
      "metadata": {
        "id": "bSfOG1aR_dCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Modeling"
      ],
      "metadata": {
        "id": "gR6rPRu8_ed5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('./train_last_3.csv')\n",
        "test = pd.read_csv('./test_last_3.csv')"
      ],
      "metadata": {
        "id": "5p-_wHd2NqDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "\n",
        "model_type = \"sileod/deberta-v3-base-tasksource-nli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "config = AutoConfig.from_pretrained(model_type)\n",
        "model_01 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)\n",
        "model_02 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)\n",
        "model_03 = AutoModelForSequenceClassification.from_pretrained(model_type, config=config)"
      ],
      "metadata": {
        "id": "XrtZvGFUNq8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_texts(texts, tokenizer, max_len=352):\n",
        "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_len, add_special_tokens=False , return_tensors='pt')\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = tokenize_texts(text, self.tokenizer, self.max_len)\n",
        "        return {'input_ids': encoding['input_ids'][0], 'attention_mask': encoding['attention_mask'][0], 'label': torch.tensor(label)}"
      ],
      "metadata": {
        "id": "MgIBahyRNvyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data_loader"
      ],
      "metadata": {
        "id": "X063yAvPNzEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_folds = [1, 2, 3]\n",
        "train_indices = []\n",
        "val_indices = []"
      ],
      "metadata": {
        "id": "asGMgwMRNydb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_index = train.iloc[list(range(0, 24780, 10))]\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_val_splits = list(skf.split(train_index, train_index['first_party_winner']))"
      ],
      "metadata": {
        "id": "QQ0_FapUN7ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k_fold in k_folds:\n",
        "    train_train =  list(train_index.iloc[train_val_splits[k_fold][0],:].index)\n",
        "    train_val =  list(train_index.iloc[train_val_splits[k_fold][1],:].index)\n",
        "\n",
        "    train_list = train_train[:]\n",
        "    val_list = train_val[:]\n",
        "\n",
        "    for num in train_train:\n",
        "        for i in range(1, 10):\n",
        "            new_value = num + i\n",
        "            train_list.append(new_value)\n",
        "\n",
        "    for num in train_val:\n",
        "        for i in range(1, 10):\n",
        "            new_value = num + i\n",
        "            val_list.append(new_value)\n",
        "\n",
        "    train_list = sorted(train_list)\n",
        "    val_list = sorted(val_list)\n",
        "\n",
        "    train_indices.append(sorted(train_list))\n",
        "    val_indices.append(sorted(val_list))"
      ],
      "metadata": {
        "id": "jTXA8mLfOAhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_01 = train.iloc[train_indices[0]]\n",
        "val_data_01 = train.iloc[val_indices[0]]\n",
        "\n",
        "train_data_02 = train.iloc[train_indices[1]]\n",
        "val_data_02 = train.iloc[val_indices[1]]\n",
        "\n",
        "train_data_03 = train.iloc[train_indices[2]]\n",
        "val_data_03 = train.iloc[val_indices[2]]"
      ],
      "metadata": {
        "id": "K_8T444qODmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_01 = NewsDataset(train_data_01['facts'].to_numpy(), train_data_01['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_01 = NewsDataset(val_data_01['facts'].to_numpy(), val_data_01['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "\n",
        "train_dataset_02 = NewsDataset(train_data_02['facts'].to_numpy(), train_data_02['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_02 = NewsDataset(val_data_02['facts'].to_numpy(), val_data_02['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "\n",
        "train_dataset_03 = NewsDataset(train_data_03['facts'].to_numpy(), train_data_03['first_party_winner'].to_numpy(), tokenizer, max_len=352)\n",
        "val_dataset_03 = NewsDataset(val_data_03['facts'].to_numpy(), val_data_03['first_party_winner'].to_numpy(), tokenizer, max_len=352)"
      ],
      "metadata": {
        "id": "MbJvuUnVOE3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_01 = DataLoader(train_dataset_01, batch_size=32, shuffle=True)\n",
        "val_loader_01 = DataLoader(val_dataset_01, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_02 = DataLoader(train_dataset_02, batch_size=32, shuffle=True)\n",
        "val_loader_02 = DataLoader(val_dataset_02, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_03 = DataLoader(train_dataset_03, batch_size=32, shuffle=True)\n",
        "val_loader_03 = DataLoader(val_dataset_03, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "dv-m2tGTOGX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "modeling"
      ],
      "metadata": {
        "id": "I1u9ruf5OIiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_01.to(device)\n",
        "model_02.to(device)\n",
        "model_03.to(device)"
      ],
      "metadata": {
        "id": "6CbFy_2mOHWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(torch.nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.1, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ],
      "metadata": {
        "id": "_C7Dg_QYOPS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_01 = AdamW(model_01.parameters(), lr=1e-5)\n",
        "optimizer_02 = AdamW(model_02.parameters(), lr=1e-5)\n",
        "optimizer_03 = AdamW(model_03.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "KKqkLabPORpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "num_classes = 2\n",
        "smoothing = 0.01\n",
        "criterion = LabelSmoothingLoss(classes=num_classes, smoothing=smoothing).to(device)"
      ],
      "metadata": {
        "id": "vKYIpHzSOUn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Training and validation\n",
        "def train(model, data_loader, optimizer, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval(model, data_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predictions.cpu().numpy())\n",
        "    return y_true, y_pred"
      ],
      "metadata": {
        "id": "FxezgqgLOWPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**kfold[1]**"
      ],
      "metadata": {
        "id": "WSHf8e3iOgw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03]\n",
        "model_loader = [model_01, model_02, model_03]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03]\n",
        "\n",
        "\n",
        "best_accuracy_01 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[0], train_loader[0], optimizer_loader[0], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[0], train_loader[0], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[0], val_loader[0], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_01:\n",
        "        best_accuracy_01 = val_accuracy\n",
        "        torch.save(model_loader[0].state_dict(), f'model_loader[0]_{k_folds[0]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "LlW9BYXfOf9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "vhdsHnI1OobE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_01.load_state_dict(torch.load(f'model_loader[0]_1.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_01, test_loader, device)\n",
        "submit['first_party_winner_01'] = test_predictions"
      ],
      "metadata": {
        "id": "xHq3bNN-Opeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_01 = submit.copy()"
      ],
      "metadata": {
        "id": "CBV8bATdOsSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_01.to_csv('submit_01_version2.csv', index = False)"
      ],
      "metadata": {
        "id": "BuAoGLUgOumN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**kfold[2]**"
      ],
      "metadata": {
        "id": "gtAfGw84Oxfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03]\n",
        "model_loader = [model_01, model_02, model_03]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03]\n",
        "\n",
        "\n",
        "best_accuracy_02 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[1], train_loader[1], optimizer_loader[1], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[1], train_loader[1], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[1], val_loader[1], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_02:\n",
        "        best_accuracy_02 = val_accuracy\n",
        "        torch.save(model_loader[1].state_dict(), f'model_loader[1]_{k_folds[1]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "uiGKP6WoOzLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "5qldaRplO2Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_02.load_state_dict(torch.load(f'model_loader[1]_2.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_02, test_loader, device)\n",
        "submit['first_party_winner_02'] = test_predictions"
      ],
      "metadata": {
        "id": "cAT2fR4ZO3iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_02 = submit.copy()"
      ],
      "metadata": {
        "id": "dNCCSitdO45b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_02.to_csv('submit_02_version2.csv', index = False)"
      ],
      "metadata": {
        "id": "tl-8nnJAO59q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**kfold[3]**"
      ],
      "metadata": {
        "id": "wfNF0kVyO75v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = [train_loader_01, train_loader_02, train_loader_03]\n",
        "val_loader = [val_loader_01, val_loader_02, val_loader_03]\n",
        "model_loader = [model_01, model_02, model_03]\n",
        "optimizer_loader = [optimizer_01, optimizer_02, optimizer_03]\n",
        "\n",
        "\n",
        "best_accuracy_03 = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    train_loss = train(model_loader[2], train_loader[2], optimizer_loader[2], device)\n",
        "    print(f\"Train Loss: {train_loss}\")\n",
        "    y_true_train, y_pred_train = eval(model_loader[2], train_loader[2], device)\n",
        "    y_true_val, y_pred_val = eval(model_loader[2], val_loader[2], device)\n",
        "\n",
        "    train_accuracy = accuracy_score(y_true_train, y_pred_train)\n",
        "    val_accuracy = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy}\")\n",
        "    print(f\"Val Accuracy: {val_accuracy}\")\n",
        "    print(classification_report(y_true_val, y_pred_val))\n",
        "\n",
        "    if val_accuracy > best_accuracy_03:\n",
        "        best_accuracy_03 = val_accuracy\n",
        "        torch.save(model_loader[2].state_dict(), f'model_loader[2]_{k_folds[2]}.pt')\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "n7T8VXj5O-nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "HzH2yy9APCv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_03.load_state_dict(torch.load(f'model_loader[2]_3.pt'))\n",
        "test_dataset = NewsDataset(test['facts'].to_numpy(), np.zeros(len(test)), tokenizer, max_len=352)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "_, test_predictions = eval(model_03, test_loader, device)\n",
        "submit['first_party_winner_03'] = test_predictions"
      ],
      "metadata": {
        "id": "ns5v4RldPEr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_03 = submit.copy()"
      ],
      "metadata": {
        "id": "T2iMcNURPGmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_03.to_csv('submit_03_version2.csv', index = False)"
      ],
      "metadata": {
        "id": "IcGu3q3cPHwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}